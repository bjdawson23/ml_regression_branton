{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Branton - Project 4\n",
    "**Author:** Branton Dawson  \n",
    "**Date:** November 13, 2025  \n",
    "**Objective:** Titanic Dataset - Predicting a Continuous Target with Regression\n",
    "\n",
    "**We will predict fare, the amount of money paid for the journey, using features on the Titanic dataset**\n",
    "\n",
    "We need a numeric target, so we will predict fare - the amount of money paid for the journey.\n",
    "\n",
    "1. Linear Regression Model\n",
    "2. Polynomial Regression Model\n",
    "3. Ridge Model\n",
    "4. Regularized Model (Elastic Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.  Import and Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'ml_regression_branton (Python 3.12.12)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Repos/ml_regression_branton/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# all imports at the top, organized\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset working with continuous targets and regression\n",
    "titanic = sns.load_dataset('titanic')\n",
    "# Display a few records to verify\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Data Exploration and Preparation\n",
    "\n",
    "### 2.1 Handle Missing Values and Clean Data\n",
    "\n",
    "Impute missing values for age using the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values for 'age' and 'embark_town' by filling with mode\n",
    "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
    "titanic = titanic.dropna(subset=['fare'])\n",
    "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n",
    "print(titanic[['age', 'fare', 'family_size']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature Engineering\n",
    "\n",
    "Create any new features that might be helpful for the model from the existing data. This is not making up new data - it is providing existing data in a more helpful, concise way for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature\n",
    "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n",
    "\n",
    "# Map categories to numeric values\n",
    "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
    "titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "titanic['alone'] = titanic['alone'].astype(int)\n",
    "titanic['class'] = titanic['class'].map({'First': 1, 'Second': 2, 'Third': 3})\n",
    "print(titanic[['age', 'fare', 'family_size', 'sex', 'embarked', 'class', 'alone']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. Feature Selection and Justification\n",
    "### 3.1 Choose features and target\n",
    "\n",
    "Case 1. age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = titanic[['age']]\n",
    "y1 = titanic['fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2. family_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = titanic[['family_size']]\n",
    "y2 = titanic['fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 3. age, family_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = titanic[['age', 'family_size']]\n",
    "y3 = titanic['fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 4. sex, class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = titanic[['sex', 'class']]\n",
    "y4 = titanic['fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Age, family_size, and fare\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(titanic['age'], bins=20, color='skyblue', alpha=0.7)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Age Distribution')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(titanic['family_size'], bins=20, color='lightgreen', alpha=0.7)\n",
    "plt.xlabel('Family Size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Family Size Distribution')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(titanic['fare'], bins=20, color='coral', alpha=0.7)\n",
    "plt.xlabel('Fare')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Fare Distribution')\n",
    "\n",
    "# plot sex vs class\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.countplot(x='sex', hue='class', data=titanic)\n",
    "plt.xlabel('Sex')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Sex vs Class Distribution')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print correlation matrix\n",
    "corr_matrix = titanic[['age', 'fare', 'family_size', 'sex', 'class']].corr()\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print heatmap of correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plot of sex and class versus fare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual sex and class vs fare\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='class', y='fare', hue='sex', data=titanic)\n",
    "plt.title('Fare Distribution by Sex and Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection 3:\n",
    "\n",
    "1. **Why might these features affect a passenger's fare:**\n",
    "   - **Age**: Older passengers might afford higher-class tickets, or families with children might book different accommodations\n",
    "   - **Family Size**: Larger families might get group discounts or need to book multiple cabins\n",
    "   - **Sex**: Gender could correlate with social status and purchasing power in 1912\n",
    "   - **Class**: Direct relationship - First class costs more than Second, which costs more than Third\n",
    "\n",
    "\n",
    "2. **List all available features:**\n",
    "\n",
    "   - `survived`, `pclass`, `sex`, `age`, `sibsp`, `parch`, `fare`, `embarked`, `class`, `who`, `adult_male`, `deck`, `embark_town`, `alive`, `alone`   \n",
    "\n",
    "3. **Which other features could improve predictions and why:**\n",
    "\n",
    "   - **Deck**: Higher decks typically indicate premium cabins with higher fares4. **How many variables are in your Case 4:** 2 variables (sex and class)\n",
    "\n",
    "   - **Embark_town**: Different ports might have different fare structures or passenger demographics\n",
    "   - **Pclass**: Most direct predictor of fare (1st, 2nd, 3rd class)\n",
    "\n",
    "4. **How many variables are in your Case 4:**  two X variables, one y variable\n",
    "\n",
    "5. **Which variable(s) did you choose for Case 4 and why:**   \n",
    "   - **sex and class**: Gender may have been a factor in affording class which would effect far \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4. Train a Regression Model (Linear Regression)\n",
    " \n",
    "### 4.1 Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=123)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=123)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=123)\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train and Evaluate Linear Regression Models (all 4 cases)\n",
    "\n",
    "We'll use a more concise approach - create each model and immediately call the fit() method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model1 = LinearRegression().fit(X1_train, y1_train)\n",
    "lr_model2 = LinearRegression().fit(X2_train, y2_train)\n",
    "lr_model3 = LinearRegression().fit(X3_train, y3_train)\n",
    "lr_model4 = LinearRegression().fit(X4_train, y4_train)\n",
    "\n",
    "# Predictions\n",
    "\n",
    "y_pred_train1 = lr_model1.predict(X1_train)\n",
    "y_pred_test1 = lr_model1.predict(X1_test)\n",
    "y_pred_train2 = lr_model2.predict(X2_train)\n",
    "y_pred_test2 = lr_model2.predict(X2_test)\n",
    "y_pred_train3 = lr_model3.predict(X3_train)\n",
    "y_pred_test3 = lr_model3.predict(X3_test)\n",
    "y_pred_train4 = lr_model4.predict(X4_train)\n",
    "y_pred_test4 = lr_model4.predict(X4_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Report Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Case 1: Training R²:\", r2_score(y1_train, y_pred_train1))\n",
    "print(\"Case 1: Test R²:\", r2_score(y1_test, y_pred_test1))\n",
    "print(\"Case 1: Test RMSE:\", np.sqrt(mean_squared_error(y1_test, y_pred_test1)))\n",
    "print(\"Case 1: Test MAE:\", mean_absolute_error(y1_test, y_pred_test1))\n",
    "print(\"-----\")\n",
    "print(\"Case 2: Training R²:\", r2_score(y2_train, y_pred_train2))\n",
    "print(\"Case 2: Test R²:\", r2_score(y2_test, y_pred_test2))\n",
    "print(\"Case 2: Test RMSE:\", np.sqrt(mean_squared_error(y2_test, y_pred_test2)))\n",
    "print(\"Case 2: Test MAE:\", mean_absolute_error(y2_test, y_pred_test2))\n",
    "print(\"-----\")\n",
    "print(\"Case 3: Training R²:\", r2_score(y3_train, y_pred_train3))\n",
    "print(\"Case 3: Test R²:\", r2_score(y3_test, y_pred_test3))\n",
    "print(\"Case 3: Test RMSE:\", np.sqrt(mean_squared_error(y3_test, y_pred_test3)))\n",
    "print(\"Case 3: Test MAE:\", mean_absolute_error(y3_test, y_pred_test3))\n",
    "print(\"-----\")\n",
    "print(\"Case 4: Training R²:\", r2_score(y4_train, y_pred_train4))\n",
    "print(\"Case 4: Test R²:\", r2_score(y4_test, y_pred_test4))\n",
    "print(\"Case 4: Test RMSE:\", np.sqrt(mean_squared_error(y4_test, y_pred_test4)))\n",
    "print(\"Case 4: Test MAE:\", mean_absolute_error(y4_test, y_pred_test4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection 4:\n",
    "\n",
    "**Compare the train vs test results for each:**\n",
    "\n",
    "- **Case 1 (Age)**: Likely **underfitting** - single feature may be too simple to capture fare patterns. Training and test R² should be similar but low.\n",
    "- **Case 2 (Family Size)**: Probably **underfitting** - family size alone doesn't strongly predict individual fares.\n",
    "- **Case 3 (Age + Family Size)**: May show slight **overfitting** if training R² > test R², but likely still underfitting overall.\n",
    "- **Case 4 (Sex + Class)**: Should perform best with minimal overfitting - class is the strongest predictor of fare.\n",
    "\n",
    "**Adding Age:**\n",
    "- **Did adding age improve the model:** Small improvement when combined with family_size (Case 3 vs Case 2)\n",
    "- **Possible explanation:** Age alone doesn't greatly affect fare on Titanic. Fare was primarily determined by ticket class and cabin location.\n",
    "\n",
    "**Worst Performance:**\n",
    "- **Which case performed worst:** Case 1 (Age only) or Case 2 (Family Size only)\n",
    "- **How do you know:** It had the lowest R² values and highest RMSE\n",
    "- **Would more training data help:** Probably not.  The relationship between age/family_size and fare is weak. \n",
    "\n",
    "**Best Performance:**\n",
    "- **Which case performed best:** Case 4 (Sex + Class) should perform best\n",
    "- **How do you know:** Highest R² and lowest RMSE values\n",
    "- **Would more training data help:** Yes. The relationship is already strong, so more data would help reduce noise and improve generalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5. Compare Alternative Models\n",
    "\n",
    "### Explore other regression models\n",
    "\n",
    "When working with regression models, especially those with multiple input features, we may run into overfitting — where a model fits the training data too closely and performs poorly on new data. To prevent this, we can apply regularization.\n",
    "\n",
    "Regularization adds a penalty to the model’s loss function, discouraging it from using very large weights (coefficients). This makes the model simpler and more likely to generalize well to new data.\n",
    "\n",
    "In general: \n",
    "\n",
    "    If the basic linear regression is overfitting, try Ridge.\n",
    "\n",
    "    If you want the model to automatically select the most important features, try Lasso.\n",
    "\n",
    "    If you want a balanced approach, try Elastic Net.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression (L2 penalty)\n",
    "\n",
    "Ridge Regression is a regularized version of linear regression that adds a penalty to large coefficient values. It uses the L2 penalty, which adds the sum of squared coefficients to the loss function.\n",
    "\n",
    "This \"shrinks\" the coefficients, reducing the model’s sensitivity to any one feature while still keeping all features in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X1_train, y1_train)\n",
    "y_pred_ridge = ridge_model.predict(X1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net (L1 + L2 combined)\n",
    "\n",
    "Lasso Regression uses the L1 penalty, which adds the sum of absolute values of the coefficients to the loss function. Lasso can shrink some coefficients all the way to zero, effectively removing less important features. This makes it useful for feature selection.\n",
    "\n",
    "    Penalty term: L1 = sum of absolute values of weights\n",
    "    Effect: Can shrink some weights to zero (drops features), simplifies the model\n",
    "\n",
    "Elastic Net combines both L1 (Lasso) and L2 (Ridge) penalties. It balances the feature selection ability of Lasso with the stability of Ridge.\n",
    "We control the balance with a parameter called l1_ratio:\n",
    "\n",
    "    If l1_ratio = 0, it behaves like Ridge\n",
    "    If l1_ratio = 1, it behaves like Lasso\n",
    "    Values in between mix both types\n",
    "    Penalty term: α × (L1 + L2)\n",
    "    Effect: Shrinks weights and can drop some features — flexible and powerful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 3. Two Inputs, Two Dimensional Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_model = ElasticNet(alpha=0.3, l1_ratio=0.5)\n",
    "elastic_model.fit(X1_train, y1_train)\n",
    "y_pred_elastic = elastic_model.predict(X1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "Linear regression is a simple two dimensional relationship - a simple straight line. But we can test more complex relationships. Polynomial regression adds interaction and nonlinear terms to the model. Be careful here - higher-degree polynomials can easily overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the poly inputs\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_train_poly = poly.fit_transform(X1_train)\n",
    "X_test_poly = poly.transform(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the poly inputs in the LR model\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train_poly, y1_train)\n",
    "y_pred_poly = poly_model.predict(X_test_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Polynomial Cubic Fit (for 1 input feature)\n",
    "\n",
    "Choose a case with just one input feature and plot it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X1_test, y1_test, color='blue', label='Actual')\n",
    "plt.scatter(X1_test, y_pred_poly, color='red', label='Predicted (Poly)')\n",
    "plt.legend()\n",
    "plt.title(\"Polynomial Regression: Age vs Fare\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Compare All Models\n",
    "\n",
    "Create a summary table or printout comparing all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(name, y_true, y_pred):\n",
    "    print(f\"{name} R²: {r2_score(y_true, y_pred):.3f}\")\n",
    "    print(f\"{name} RMSE: {np.sqrt(mean_squared_error(y_true, y_pred)):.2f}\")\n",
    "    print(f\"{name} MAE: {mean_absolute_error(y_true, y_pred):.2f}\\n\")\n",
    "\n",
    "report(\"Linear\", y1_test, y_pred_test1)\n",
    "report(\"Ridge\", y1_test, y_pred_ridge)\n",
    "report(\"ElasticNet\", y1_test, y_pred_elastic)\n",
    "report(\"Polynomial\", y1_test, y_pred_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | R² | RMSE | MAE |\n",
    "|----------------------------------|-----|-----|-----|\n",
    "| Linear Regression | 0.003 | 37.97 | 25.29 |\n",
    "| Ridge | 0.003 | 37.97 | 25.29 |\n",
    "| Polynomial Regression (degree 3) | 0.003 | 37.97 | 25.28 |\n",
    "| Elastic Net | -0.003 | 38.10 | 25.30 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection 5.4:\n",
    "\n",
    "1. **What patterns does the cubic model seem to capture:**\n",
    "   - **Non-linear age-fare relationships**: Captures curves where some age groups might pay different fares\n",
    "   - Cubic allows for 2 direction changes.\n",
    "\n",
    "2. **Where does it perform well or poorly:**\n",
    "\n",
    "   - **Performs well**: Middle age ranges where there's sufficient data density  \n",
    "   Outliers and passengers who paid unusual fares regardless of age   \n",
    "\n",
    "3. **Did the polynomial fit outperform linear regression:**\n",
    "\n",
    "   - **Likely modest improvement**: Polynomial should show slightly better R² on training data   \n",
    "   \n",
    "4. **Where does it fit best:**  Ages 20-40 where most passengers are located\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Visualize Higher Order Polynomial (for the same 1 input case)\n",
    "\n",
    "Use the same single input case as you visualized above, but use a higher degree polynomial (e.g. 4, 5, 6, 7, or 8). Plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Higher Order Polynomial (for the same 1 input case)\n",
    "# use degree 6 for this example\n",
    "poly_high = PolynomialFeatures(degree=6)\n",
    "X_train_poly_high = poly_high.fit_transform(X1_train)\n",
    "X_test_poly_high = poly_high.transform(X1_test)\n",
    "poly_model_high = LinearRegression()\n",
    "poly_model_high.fit(X_train_poly_high, y1_train)\n",
    "y_pred_poly_high = poly_model_high.predict(X_test_poly_high)\n",
    "plt.scatter(X1_test, y1_test, color='blue', label='Actual')\n",
    "plt.scatter(X1_test, y_pred_poly_high, color='red', label='Predicted (Poly Degree 6)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflection on Higher Order Polynomial\n",
    "\n",
    "\n",
    "**Which option works better - cubic (degree 3) or higher order (degree 6)?****\n",
    "\n",
    "**Key Insight**: More complex doesn't mean better - the cubic model likely captures the essential age-fare relationship without overfitting to noise.\n",
    "\n",
    "**Cubic (degree 3) likely performs better** for several reasons:- RMSE is higher for degree 6 on test data\n",
    "\n",
    "- Degree 6 shows erratic predictions on the scatter plot\n",
    "\n",
    "1. **Overfitting Prevention**: Degree 6 creates 6 coefficients from single feature, likely overfitting the training data- Training R² improves with degree 6, but test R² decreases\n",
    "\n",
    "2. **Generalization**: Lower degree polynomials generalize better to unseen data**Evidence to look for:**\n",
    "\n",
    "3. **Data Limitations**: With limited Titanic data (~800 passengers), high-degree polynomials fit noise rather than signal\n",
    "\n",
    "4. **Interpretability**: Cubic relationships are more interpretable than 6th-degree curves5. **Stability**: Higher-degree polynomials can create wild oscillations between data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6. Final Thoughts & Insights\n",
    "\n",
    "### 6.1 Summarize Findings\n",
    "\n",
    "1. **What features were most useful?**\n",
    "   - **Class and Sex (Case 4)**: Most predictive combination - class directly determines fare structure, sex correlates with 1912 social/economic patterns\n",
    "   - **Class alone**: Single strongest predictor (First > Second > Third class fares)\n",
    "   \n",
    "2. **What regression model performed best?**\n",
    "   - **Linear Regression with Class+Sex**: Simple linear relationship captures fare structure effectively\n",
    "  \n",
    "3. **How did model complexity or regularization affect results?**\n",
    "   - **Simple linear models**: Performed well due to straightforward fare-class relationship\n",
    "   \n",
    "### 6.2 Discuss Challenges\n",
    "\n",
    "1. **Was fare hard to predict? Why?**\n",
    "   - **Reasons for difficulty**: There were few expensive tickets making for heavy outliers\n",
    "\n",
    "2. **Did skew or outliers impact the models?**\n",
    "\n",
    "   - **Fare distribution skew**: Right-skewed (few very expensive tickets) affects linear regression assumptions   \n",
    "\n",
    "   - **Age gaps**: Missing age data and age extremes created prediction challenges   \n",
    "\n",
    "### 6.3 Next Steps for Improvement\n",
    "\n",
    "1. **Feature Engineering:**   - Handle outliers (>95th percentile fares) separately\n",
    "\n",
    "   - Add `pclass` directly (strongest predictor)   - Log-transform fare to reduce right skew\n",
    "\n",
    "   - Create fare categories instead of continuous prediction2. \n",
    "2. **Try predicting age instead of fare**\n",
    "   \n",
    "   - Create age bins instead of continuous age\n",
    "\n",
    "3. **Explore log transformation of fare to reduce skew**\n",
    "\n",
    "   - Combine `embarked` with `class` for port-specific pricing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_regression_branton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
